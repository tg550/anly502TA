{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark Exercise (20 points)\n",
    "\n",
    "**Read through the entire workbook before beginning work to familiarize with what is being asked.**\n",
    "\n",
    "In this exericise, you will work with a fairly large dataset and do some modeling and predictions using Spark Machine Learning. The dataset being used is the 2013 NYC Taxi Dataset [https://archive.org/details/nycTaxiTripData2013](https://archive.org/details/nycTaxiTripData2013). The original dataset consists of two files, `trip_data` and `trip_fare`, which need to be merged to create the dataset for modeling. \n",
    "\n",
    "The merge has already been performed and stored as a parquet file on S3 because there were some data issues that were not covered in class. The workbook [merge-fare-and-trip-data-create-parquet.ipynb](merge-fare-and-trip-data-create-parquet.ipynb) contains code for creating the merged set (for reference.)\n",
    "\n",
    "This is an interactive PySpark session. Remember that when you open this notebook the SparkContext and SparkSession are already created, and they are in the sc and spark variables, respectively. You can run the following two cells to make sure that the Kernel is active.\n",
    "\n",
    "**Do not insert any additional cells than the ones that are provided for assignment submission. You may add cells as you work through the notebook, but you need to delete extra cells and keep the initial structure.**\n",
    "\n",
    "## Some tips you may want to use:\n",
    "\n",
    "* You may consider saving intermediate datasets (i.e. training and testing) when you first create them. To save intermediate datasets, save them as **parquet** files in your own S3 bucket. To save, use the following code: `df.write.parquet(\"s3://[[your-s3-bucket]]/data_location/\", mode=\"overwrite\")`.\n",
    "* You may also want to save a model object in S3 after you train it, especially if training takes a while. To save a model object, use the following code: `model.save(\"s3://[[your-s3-bucket]]/model_location/\")`\n",
    "* When creating the Machine Learning pipelines, you may want to try it first on a minuscule sample of your training data to make sure the pipelines work as planned. To create a tiny DataFrame, use the `limit` method: `df.limit(100)` (this creates a small DataFrame with the first 100 rows from df.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-49-200.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-49-200.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdd2c32f950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries\n",
    "\n",
    "The following cell will load all required libraries and functions for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.feature import RFormula\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Load the data, convert data types and create new features (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "In the following cell, create an object called `nyctaxi` which loads the **parquet** files from `s3://bigdatateaching/nyctaxi-2013/merged-parquet/`.\n",
    "\n",
    "This step took approximately 8 seconds during the development of the workbook using the cluster configuration in the assignment. The workbook will have spots were running times during development are written for reference as a comment in a cell. Your run times may vary, but these values will give you an indication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 8 seconds\n",
    "nyctaxi = spark.read.parquet(\"s3://bigdatateaching/nyctaxi-2013/merged-parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell print the schema of `nyctaxi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- surcharge: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total: string (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_time_in_secs: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyctaxi.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, count the number of records for `nyctaxi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173185091"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~ 12 seconds\n",
    "nyctaxi.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, show the first 10 records of `nyctaxi` to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|           medallion|        hack_license|vendor_id|    pickup_datetime|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|rate_code|store_and_fwd_flag|   dropoff_datetime|passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|\n",
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|00005007A9F30E289...|132A7AC13C8471488...|      CMT|2013-07-30 22:50:31|         CRD|       10.5|      0.5|    0.5|         3|           0| 14.5|        1|                 N|2013-07-30 23:01:50|              1|              679|         2.50|      -73.981644|      40.752388|       -73.982964|       40.775761|\n",
      "|00005007A9F30E289...|16780B3E72BAA7A5C...|      CMT|2013-07-16 19:22:49|         CSH|        2.5|        1|    0.5|         0|           0|    4|        1|                 N|2013-07-16 19:40:47|              1|             1078|          .00|       -73.98587|      40.759899|       -73.985451|       40.747894|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-11 11:18:25|         CRD|        5.5|        0|    0.5|         1|           0|    7|        1|                 Y|2013-10-11 11:23:11|              1|              285|          .90|      -73.997002|       40.72245|       -73.986115|       40.729259|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-21 07:45:29|         CRD|        7.5|        0|    0.5|       1.1|           0|  9.1|        1|                 N|2013-10-21 07:52:41|              1|              432|         1.70|      -73.958084|      40.778934|       -73.975479|       40.760258|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-21 16:23:04|         CSH|        5.5|        1|    0.5|         0|           0|    7|        1|                 N|2013-10-21 16:28:01|              3|              297|          .90|       -73.97213|      40.793816|       -73.962753|       40.804379|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-23 06:40:22|         CRD|         10|        0|    0.5|       2.1|           0| 12.6|        1|                 N|2013-10-23 06:48:34|              1|              491|         2.80|      -73.973419|      40.784752|       -73.982384|       40.755123|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-30 09:52:19|         CRD|          6|        0|    0.5|       1.3|           0|  7.8|        1|                 N|2013-10-30 09:59:08|              1|              409|          .70|      -73.980362|      40.751549|       -73.985512|       40.743267|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-30 14:32:32|         CSH|        7.5|        0|    0.5|         0|           0|    8|        1|                 N|2013-10-30 14:41:54|              1|              561|          .90|       -73.98616|      40.755646|       -73.969231|       40.761013|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-11-07 07:40:09|         CSH|        2.5|        0|    0.5|         0|           0|    3|        1|                 N|2013-11-07 07:45:16|              1|              307|          .00|      -73.956886|       40.77837|       -73.970078|       40.764481|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-11-07 10:26:14|         CRD|          8|        0|    0.5|         1|           0|  9.5|        1|                 N|2013-11-07 10:37:14|              1|              660|         1.00|       -74.00441|      40.752228|       -73.992271|       40.755032|\n",
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ~ 4 seconds\n",
    "nyctaxi.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data types\n",
    "\n",
    "As you can see from printing the `nyctaxi` schema, all of the fields were loaded as strings. This will not work for modeling purposes. You need to convert some of the fields to other types. In the following cell, create a new DataFrame called `nyctaxi_converted` and use the `withColumn` method to do the following conversions:\n",
    "\n",
    " Field Name | Type\n",
    "------------|-----\n",
    "trip_time_in_seconds | integer\n",
    "trip_distance | float\n",
    "pickup_latitude | float\n",
    "pickup_longitude | float\n",
    "dropoff_latitude | float\n",
    "dropoff_longitude | float\n",
    "fare_amount | float\n",
    "surcharge | float\n",
    "mta_tax | float\n",
    "tip_amount | float\n",
    "tolls_amount | float\n",
    "total | float\n",
    "pickup_datetime | timestamp\n",
    "dropoff_datetime | timestamp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxi_converted = \\\n",
    "(nyctaxi\n",
    "     .withColumn(\"trip_time_in_secs\", nyctaxi.trip_time_in_secs.cast(\"float\"))\n",
    "     .withColumn(\"trip_distance\", nyctaxi.trip_distance.cast(\"float\"))\n",
    "     .withColumn(\"pickup_latitude\", nyctaxi.pickup_latitude.cast(\"float\"))\n",
    "     .withColumn(\"pickup_longitude\", nyctaxi.pickup_longitude.cast(\"float\"))                \n",
    "     .withColumn(\"dropoff_latitude\", nyctaxi.dropoff_latitude.cast(\"float\"))               \n",
    "     .withColumn(\"dropoff_longitude\", nyctaxi.dropoff_longitude.cast(\"float\")) \n",
    "     .withColumn(\"fare_amount\", nyctaxi.fare_amount.cast(\"float\"))\n",
    "     .withColumn(\"surcharge\", nyctaxi.surcharge.cast(\"float\")) \n",
    "     .withColumn(\"mta_tax\", nyctaxi.mta_tax.cast(\"float\")) \n",
    "     .withColumn(\"tip_amount\", nyctaxi.tip_amount.cast(\"float\")) \n",
    "     .withColumn(\"tolls_amount\", nyctaxi.tolls_amount.cast(\"float\"))\n",
    "     .withColumn(\"total\", nyctaxi.total.cast(\"float\"))\n",
    "     .withColumn(\"pickup_datetime\", nyctaxi.pickup_datetime.cast(\"timestamp\"))\n",
    "     .withColumn(\"dropoff_datetime\", nyctaxi.dropoff_datetime.cast(\"timestamp\"))\n",
    "     .withColumn(\"passenger_count\", nyctaxi.passenger_count.cast(\"integer\"))\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will add a few new fields with data derived from current fields as new features.\n",
    "\n",
    "In the following cell, add the following columns to `nyctaxi_converted` using the `withColumn` method:\n",
    "\n",
    "* A column called `pickup_hour` with the hour from `pickup_datetime`. This provides an integer from 0 to 23.\n",
    "* A column called `pickup_week` with the week of the year from `pickup_datetime`. This provides an integer from 1 to 53.\n",
    "* A column called `weekday` with the name of the day of the week ffrom `pickup_datetime`, in long format.\n",
    "* A column called `tipped` which is an indicator of wether or not there was a tip. If the tip is 0, then it's 0, otherwise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tip(entry):\n",
    "    if entry != 0:\n",
    "        z = 1\n",
    "    else:\n",
    "        z = 0\n",
    "    return z\n",
    "\n",
    "tip1 = udf(tip, \"integer\")\n",
    "\n",
    "nyctaxi_converted = \\\n",
    "(nyctaxi_converted\n",
    "     .withColumn(\"pickup_hour\", hour(nyctaxi_converted.pickup_datetime).cast(\"integer\"))\n",
    "     .withColumn(\"pickup_week\", weekofyear(nyctaxi_converted.pickup_datetime).cast(\"integer\"))\n",
    "     .withColumn(\"weekday\", date_format(nyctaxi_converted.pickup_datetime, 'EEEE'))\n",
    "     .withColumn(\"tipped\", tip1(nyctaxi_converted.tip_amount))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the creation of the next feature, you will use a SparkSQL statement. Therefore, you need to register your DataFrame. In the next cell, register the `nyctaxi_converted` DataFrame as `nyctaxi_converted_tbl.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxi_converted.createOrReplaceTempView(\"nyctaxi_converted_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, you will add a new column called `time_bins` that takes the value of `pickup_hour` and buckets it according to the following rules. You must assign the SQL statement back to `nyctaxi_converted`:\n",
    "\n",
    "* If the value of the pickup hour is at-or-before 6am, or at-or-after 8pm, then the value is \"night\"\n",
    "* If the value of the pickup hour is between 7am and 10am (inclusive), then the value is \"am_rush\"\n",
    "* If the value of the pickup hour is between 11am and 3pm (inclusive), then the value is \"afternoon\"\n",
    "* If the value of the pickup hour is between 4pm and 7pm (inclusive), then the value is \"pm_rush\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxi_converted = spark.sql(\"\"\"\n",
    "select *\n",
    ",CASE WHEN pickup_hour <= 6 or pickup_hour >= 20 THEN 'night'\n",
    "                WHEN pickup_hour >= 7 and pickup_hour <= 10 THEN 'am_rush'\n",
    "                WHEN pickup_hour >= 11 and pickup_hour <= 15 THEN 'afternoon'\n",
    "                WHEN pickup_hour >= 16 and pickup_hour <= 19 THEN 'pm_rush'\n",
    "                END as time_bins\n",
    "from nyctaxi_converted_tbl\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, print the schema of your `nyctaxi_converted` DataFrame and make sure it compares to the results below. Field order is not important.\n",
    "\n",
    "```\n",
    "root\n",
    " |-- medallion: string (nullable = true)\n",
    " |-- hack_license: string (nullable = true)\n",
    " |-- vendor_id: string (nullable = true)\n",
    " |-- pickup_datetime: timestamp (nullable = true)\n",
    " |-- payment_type: string (nullable = true)\n",
    " |-- fare_amount: float (nullable = true)\n",
    " |-- surcharge: float (nullable = true)\n",
    " |-- mta_tax: float (nullable = true)\n",
    " |-- tip_amount: float (nullable = true)\n",
    " |-- tolls_amount: float (nullable = true)\n",
    " |-- total: float (nullable = true)\n",
    " |-- rate_code: string (nullable = true)\n",
    " |-- store_and_fwd_flag: string (nullable = true)\n",
    " |-- dropoff_datetime: timestamp (nullable = true)\n",
    " |-- passenger_count: integer (nullable = true)\n",
    " |-- trip_time_in_secs: float (nullable = true)\n",
    " |-- trip_distance: float (nullable = true)\n",
    " |-- pickup_longitude: float (nullable = true)\n",
    " |-- pickup_latitude: float (nullable = true)\n",
    " |-- dropoff_longitude: float (nullable = true)\n",
    " |-- dropoff_latitude: float (nullable = true)\n",
    " |-- pickup_hour: integer (nullable = true)\n",
    " |-- pickup_week: integer (nullable = true)\n",
    " |-- weekday: string (nullable = true)\n",
    " |-- tipped: integer (nullable = false)\n",
    " |-- time_bins: string (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- surcharge: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total: float (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: float (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: float (nullable = true)\n",
      " |-- pickup_latitude: float (nullable = true)\n",
      " |-- dropoff_longitude: float (nullable = true)\n",
      " |-- dropoff_latitude: float (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pickup_week: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- tipped: integer (nullable = true)\n",
      " |-- time_bins: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyctaxi_converted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, show the first 10 rows of `nyctaxi_converted` to see if your new fields are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+-----------+-----------+---------+------+---------+\n",
      "|           medallion|        hack_license|vendor_id|    pickup_datetime|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total|rate_code|store_and_fwd_flag|   dropoff_datetime|passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|pickup_hour|pickup_week|  weekday|tipped|time_bins|\n",
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+-----------+-----------+---------+------+---------+\n",
      "|00005007A9F30E289...|132A7AC13C8471488...|      CMT|2013-07-30 22:50:31|         CRD|       10.5|      0.5|    0.5|       3.0|         0.0| 14.5|        1|                 N|2013-07-30 23:01:50|              1|            679.0|          2.5|       -73.98164|      40.752388|        -73.98296|        40.77576|         22|         31|  Tuesday|     1|    night|\n",
      "|00005007A9F30E289...|16780B3E72BAA7A5C...|      CMT|2013-07-16 19:22:49|         CSH|        2.5|      1.0|    0.5|       0.0|         0.0|  4.0|        1|                 N|2013-07-16 19:40:47|              1|           1078.0|          0.0|       -73.98587|        40.7599|        -73.98545|       40.747894|         19|         29|  Tuesday|     0|  pm_rush|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-11 11:18:25|         CRD|        5.5|      0.0|    0.5|       1.0|         0.0|  7.0|        1|                 Y|2013-10-11 11:23:11|              1|            285.0|          0.9|         -73.997|       40.72245|       -73.986115|        40.72926|         11|         41|   Friday|     1|afternoon|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-21 07:45:29|         CRD|        7.5|      0.0|    0.5|       1.1|         0.0|  9.1|        1|                 N|2013-10-21 07:52:41|              1|            432.0|          1.7|      -73.958084|      40.778934|        -73.97548|       40.760258|          7|         43|   Monday|     1|  am_rush|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-21 16:23:04|         CSH|        5.5|      1.0|    0.5|       0.0|         0.0|  7.0|        1|                 N|2013-10-21 16:28:01|              3|            297.0|          0.9|       -73.97213|      40.793816|        -73.96275|        40.80438|         16|         43|   Monday|     0|  pm_rush|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-23 06:40:22|         CRD|       10.0|      0.0|    0.5|       2.1|         0.0| 12.6|        1|                 N|2013-10-23 06:48:34|              1|            491.0|          2.8|       -73.97342|       40.78475|        -73.98238|       40.755123|          6|         43|Wednesday|     1|    night|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-30 09:52:19|         CRD|        6.0|      0.0|    0.5|       1.3|         0.0|  7.8|        1|                 N|2013-10-30 09:59:08|              1|            409.0|          0.7|       -73.98036|       40.75155|        -73.98551|       40.743267|          9|         44|Wednesday|     1|  am_rush|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-10-30 14:32:32|         CSH|        7.5|      0.0|    0.5|       0.0|         0.0|  8.0|        1|                 N|2013-10-30 14:41:54|              1|            561.0|          0.9|       -73.98616|      40.755646|        -73.96923|       40.761013|         14|         44|Wednesday|     0|afternoon|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-11-07 07:40:09|         CSH|        2.5|      0.0|    0.5|       0.0|         0.0|  3.0|        1|                 N|2013-11-07 07:45:16|              1|            307.0|          0.0|       -73.95689|       40.77837|        -73.97008|        40.76448|          7|         45| Thursday|     0|  am_rush|\n",
      "|00005007A9F30E289...|24C122A944FB8EE21...|      CMT|2013-11-07 10:26:14|         CRD|        8.0|      0.0|    0.5|       1.0|         0.0|  9.5|        1|                 N|2013-11-07 10:37:14|              1|            660.0|          1.0|       -74.00441|      40.752228|        -73.99227|        40.75503|         10|         45| Thursday|     1|  am_rush|\n",
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+-----+---------+------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+-----------+-----------+---------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ~ 12 seconds\n",
    "nyctaxi_converted.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Drop unused variables and filter data for clean trips (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, create a new DataFrame called `valid_trips` with the following structure. You are welcome to use DataFrame methods or SparkSQL for this.\n",
    "\n",
    "* Keep only these fields: time_bins, tipped, weekday, pickup_week, pickup_hour, trip_distance, trip_time_in_secs, passenger_count, rate_code, total, tip_amount, fare_amount, payment_type, vendor_id\n",
    "* Filter records on the following criteria:\n",
    "  * Passenger count is greater than 0 and less than 8\n",
    "  * Payment type is cash (CSH) or credit card (CRD)\n",
    "  * Tip amount is 0 or more, but less than 30\n",
    "  * Fare amount is between \\\\$1 or more, and less than \\\\$150\n",
    "  * Trip distance is less than 100 miles, but more than 0\n",
    "  * The time of the trip is 30 seconds or more, and less than 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyctaxi_converted.createOrReplaceTempView(\"nyctaxi_converted_new\")\n",
    "\n",
    "valid_trips = spark.sql(\"\"\"\n",
    "select time_bins, tipped, weekday, pickup_week, pickup_hour, trip_distance, trip_time_in_secs, passenger_count\n",
    ",rate_code, total, tip_amount, fare_amount, payment_type, vendor_id\n",
    "from nyctaxi_converted_new\n",
    "where (passenger_count > 0 and passenger_count < 8)\n",
    "and (payment_type = 'CSH' or payment_type = 'CRD')\n",
    "and (tip_amount >= 0 and tip_amount < 30)\n",
    "and (fare_amount >= 1 and fare_amount < 150)\n",
    "and (trip_distance > 0 and trip_distance < 100)\n",
    "and (trip_time_in_secs >= 30 and trip_time_in_secs < 7200)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, print the schema of the `valid_trips` DataFrame and compare to this:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- vendor_id: string (nullable = true)\n",
    " |-- payment_type: string (nullable = true)\n",
    " |-- fare_amount: float (nullable = true)\n",
    " |-- tip_amount: float (nullable = true)\n",
    " |-- total: float (nullable = true)\n",
    " |-- rate_code: string (nullable = true)\n",
    " |-- passenger_count: integer (nullable = true)\n",
    " |-- trip_time_in_secs: float (nullable = true)\n",
    " |-- trip_distance: float (nullable = true)\n",
    " |-- pickup_hour: integer (nullable = true)\n",
    " |-- pickup_week: integer (nullable = true)\n",
    " |-- weekday: string (nullable = true)\n",
    " |-- tipped: integer (nullable = false)\n",
    " |-- time_bins: string (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time_bins: string (nullable = true)\n",
      " |-- tipped: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- pickup_week: integer (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- trip_time_in_secs: float (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- total: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, count the number of records for `valid_trips`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171182914"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~ 48 seconds\n",
    "valid_trips.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, calculate the number of records that were dropped with the filter applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2002177"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyctaxi_converted.count() - valid_trips.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Feature transformation and data preparation for modeling (7 points)\n",
    "\n",
    "In this section, you will convert the categorical variables `vendor_id`, `rate_code`, `payment_type`, and `time_bins` to features that can be used in the model. You need to use the `StringIndexer` function on each of the categorical variables to convert the raw text into indices, and then for each set of indices you need to use the `OneHotEncoder` function to convert the index to a vector of dummy variables.\n",
    "\n",
    "In the next 8 cells, create four `StringIndexer` objects named `si_1` through `si_4`, and four `OneHotEncoder` objects named `en_1` through `en_4` for each of the categorical variables. Each `en_` must use as input, the output from the `si_`. The four output columns for `en_` must be named `vendor_vec`, `rate_vec`, `payment_vec`, `time_bins_vec`.\n",
    "\n",
    "Make sure that you use the parameter `dropLast=False` for `OneHotEncoder`, and that the `en_` and `si_` of the same number are used on the same variable.\n",
    "\n",
    "We talked about transformers and estimators in class. Refer to the book for additional information. As a brief summary:\n",
    "* Transformers: take data in, and produce new data\n",
    "* Estimators: take data in and produce a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_1 = StringIndexer(inputCol=\"vendor_id\", outputCol=\"vendor_id_Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_2 = StringIndexer(inputCol=\"rate_code\", outputCol=\"rate_code_Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_3 = StringIndexer(inputCol=\"payment_type\", outputCol=\"payment_type_Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_4 = StringIndexer(inputCol=\"time_bins\", outputCol=\"time_bins_Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_1 = OneHotEncoder(inputCol=\"vendor_id_Index\", outputCol=\"vendor_vec\", dropLast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_2 = OneHotEncoder(inputCol=\"rate_code_Index\", outputCol=\"rate_vec\", dropLast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_3 = OneHotEncoder(inputCol=\"payment_type_Index\", outputCol=\"payment_vec\", dropLast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_4 = OneHotEncoder(inputCol=\"time_bins_Index\", outputCol=\"time_bins_vec\", dropLast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, build a pipeline called `encoded_final` where you will run the stages in the following order: si_1, en_1, si_2, en_2, etc. You need to run a fit method and a transform method on the `vaild_trips` DataFrame in order to get the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 8 min\n",
    "pipeline = Pipeline(stages=[si_1, en_1, si_2, en_2, si_3, en_3, si_4, en_4])\n",
    "to_fit = pipeline.fit(valid_trips)\n",
    "encoded_final = to_fit.transform(valid_trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, show the first 10 rows of `encoded_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---------+-----------+-----------+-------------+-----------------+---------------+---------+-----+----------+-----------+------------+---------+---------------+-------------+---------------+--------------+------------------+-------------+---------------+-------------+\n",
      "|time_bins|tipped|  weekday|pickup_week|pickup_hour|trip_distance|trip_time_in_secs|passenger_count|rate_code|total|tip_amount|fare_amount|payment_type|vendor_id|vendor_id_Index|   vendor_vec|rate_code_Index|      rate_vec|payment_type_Index|  payment_vec|time_bins_Index|time_bins_vec|\n",
      "+---------+------+---------+-----------+-----------+-------------+-----------------+---------------+---------+-----+----------+-----------+------------+---------+---------------+-------------+---------------+--------------+------------------+-------------+---------------+-------------+\n",
      "|    night|     1|  Tuesday|         31|         22|          2.5|            679.0|              1|        1| 14.5|       3.0|       10.5|         CRD|      CMT|            1.0|(2,[1],[1.0])|            0.0|(20,[0],[1.0])|               0.0|(2,[0],[1.0])|            0.0|(4,[0],[1.0])|\n",
      "|afternoon|     1|   Friday|         41|         11|          0.9|            285.0|              1|        1|  7.0|       1.0|        5.5|         CRD|      CMT|            1.0|(2,[1],[1.0])|            0.0|(20,[0],[1.0])|               0.0|(2,[0],[1.0])|            1.0|(4,[1],[1.0])|\n",
      "|  am_rush|     1|   Monday|         43|          7|          1.7|            432.0|              1|        1|  9.1|       1.1|        7.5|         CRD|      CMT|            1.0|(2,[1],[1.0])|            0.0|(20,[0],[1.0])|               0.0|(2,[0],[1.0])|            3.0|(4,[3],[1.0])|\n",
      "|  pm_rush|     0|   Monday|         43|         16|          0.9|            297.0|              3|        1|  7.0|       0.0|        5.5|         CSH|      CMT|            1.0|(2,[1],[1.0])|            0.0|(20,[0],[1.0])|               1.0|(2,[1],[1.0])|            2.0|(4,[2],[1.0])|\n",
      "|    night|     1|Wednesday|         43|          6|          2.8|            491.0|              1|        1| 12.6|       2.1|       10.0|         CRD|      CMT|            1.0|(2,[1],[1.0])|            0.0|(20,[0],[1.0])|               0.0|(2,[0],[1.0])|            0.0|(4,[0],[1.0])|\n",
      "|  am_rush|     1|Wednesday|         44|          9|          0.7|            409.0|              1|        1|  7.8|       1.3|        6.0|         CRD|      CMT|            1.0|(2,[1],[1.0])|            0.0|(20,[0],[1.0])|               0.0|(2,[0],[1.0])|            3.0|(4,[3],[1.0])|\n",
      "|afternoon|     0|Wednesday|         44|         14|          0.9|            561.0|              1|        1|  8.0|       0.0|        7.5|         CSH|      CMT|            1.0|(2,[1],[1.0])|            0.0|(20,[0],[1.0])|               1.0|(2,[1],[1.0])|            1.0|(4,[1],[1.0])|\n",
      "|  am_rush|     1| Thursday|         45|         10|          1.0|            660.0|              1|        1|  9.5|       1.0|        8.0|         CRD|      CMT|            1.0|(2,[1],[1.0])|            0.0|(20,[0],[1.0])|               0.0|(2,[0],[1.0])|            3.0|(4,[3],[1.0])|\n",
      "|afternoon|     1| Thursday|         50|         14|          0.6|            248.0|              1|        1|  6.0|       1.0|        4.5|         CRD|      CMT|            1.0|(2,[1],[1.0])|            0.0|(20,[0],[1.0])|               0.0|(2,[0],[1.0])|            1.0|(4,[1],[1.0])|\n",
      "|  am_rush|     0|Wednesday|         51|          9|          2.4|            716.0|              2|        1| 11.5|       0.0|       11.0|         CSH|      CMT|            1.0|(2,[1],[1.0])|            0.0|(20,[0],[1.0])|               1.0|(2,[1],[1.0])|            3.0|(4,[3],[1.0])|\n",
      "+---------+------+---------+-----------+-----------+-------------+-----------------+---------------+---------+-----+----------+-----------+------------+---------+---------------+-------------+---------------+--------------+------------------+-------------+---------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ~ 30 sec\n",
    "encoded_final.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, split `encoded_final` into `train` and `test` using 90% train, 10% test and a seed of 12345. **You must use the specified seed for reproducibility.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = encoded_final.randomSplit([0.9, 0.1], seed=12345)\n",
    "train = split[0]\n",
    "test = split[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, cache the `train` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[time_bins: string, tipped: int, weekday: string, pickup_week: int, pickup_hour: int, trip_distance: float, trip_time_in_secs: float, passenger_count: int, rate_code: string, total: float, tip_amount: float, fare_amount: float, payment_type: string, vendor_id: string, vendor_id_Index: double, vendor_vec: vector, rate_code_Index: double, rate_vec: vector, payment_type_Index: double, payment_vec: vector, time_bins_Index: double, time_bins_vec: vector]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, cache the `test` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[time_bins: string, tipped: int, weekday: string, pickup_week: int, pickup_hour: int, trip_distance: float, trip_time_in_secs: float, passenger_count: int, rate_code: string, total: float, tip_amount: float, fare_amount: float, payment_type: string, vendor_id: string, vendor_id_Index: double, vendor_vec: vector, rate_code_Index: double, rate_vec: vector, payment_type_Index: double, payment_vec: vector, time_bins_Index: double, time_bins_vec: vector]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of records in the `train` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154064279"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~ 10 min - this takes time because you are actually loading the train dataset into memory as you count. \n",
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Build a Logistic Regression Model to predict tipping. (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section, you will train a Logistic Regression model to predict wether or not there was a tip for each ride using the training data created in the previous section. You will build pipelines using both transformers and estimators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, create a `LogisticRegression` estimator called `log_reg` with the following parameters: `maxIter = 10, regParam = 0.3, eslaticNetParam = 0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(maxIter=10, regParam = 0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model you will be fitting is predicting `tipped` as a function of:\n",
    "* Pickup hour\n",
    "* Passenger count\n",
    "* Trip time\n",
    "* Trip distance\n",
    "* Fare amount\n",
    "* Vendor id\n",
    "* Payment type\n",
    "* Rate code\n",
    "* Time bins\n",
    "\n",
    "Remember, you can't use the raw categorical data, you need to use something else for the categorical variables.\n",
    "\n",
    "In the next cell, create an object called `class_formula` and build a formula for the regression based on the previous instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_formula = RFormula(\n",
    "    formula=\"tipped ~ pickup_hour + passenger_count + trip_time_in_secs + trip_distance + \\\n",
    "                        fare_amount + vendor_vec + payment_vec + rate_vec + time_bins_vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, create a PipelineModel object called model, with the stages of `class_formula` and `log_reg`, in that order. Run the fit method on the `train` DataFrame. This creates a transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o683.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 44 in stage 30.0 failed 4 times, most recent failure: Lost task 44.3 in stage 30.0 (TID 1197, ip-172-31-61-36.ec2.internal, executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 5.5 GB of 5.5 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1708)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1695)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1695)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1867)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\n\tat org.apache.spark.ml.classification.LogisticCostFun.calculate(LogisticRegression.scala:1894)\n\tat org.apache.spark.ml.classification.LogisticCostFun.calculate(LogisticRegression.scala:1869)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:23)\n\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:55)\n\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:48)\n\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:89)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:782)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:487)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:278)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-43afb74aefff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_formula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o683.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 44 in stage 30.0 failed 4 times, most recent failure: Lost task 44.3 in stage 30.0 (TID 1197, ip-172-31-61-36.ec2.internal, executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 5.5 GB of 5.5 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1708)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1695)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1695)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1867)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\n\tat org.apache.spark.ml.classification.LogisticCostFun.calculate(LogisticRegression.scala:1894)\n\tat org.apache.spark.ml.classification.LogisticCostFun.calculate(LogisticRegression.scala:1869)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:23)\n\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:55)\n\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:48)\n\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:89)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:782)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:487)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:278)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "pip = Pipeline(stages=[class_formula, log_reg])\n",
    "model = pip.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, create a `predictions` DataFrame by taking the model produced in the previous cell and running the `transform` method on the `test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, create an object called `predictions_and_labels` which takes the `label` and `prediction` columns from the `predictions` DataFrame and converts it to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, create an evaluator called metrics, and use the `BinaryClassificationMetrics` evaluator on the `predictions_and_labels` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last step, the code is there for you to run. The next cell calculates the Area Under the Curve for the ROC curve generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~13 min\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the result of the Area under ROC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type answer here. Do not evaluate this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit (5 points)\n",
    "\n",
    "For extra credit, take the `label` and `probability` columns from the `predictions` DataFrame and create a Pandas DataFrame (local to the master node), and plot a ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, make a local Pandas DataFrame from `predictions`. **Hint:** you may want to randomly sample about 20% of the results, not use the entire `predictions`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, use `matplotlib` and `scikit-learn` to plot the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (For Reference) Load a saved pipeline model and evaluate it on a data set\n",
    "\n",
    "If you did save a model as suggested in the beginning of the workbook, you can load it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "saved_model = PipelineModel.load(\"s3://[[your-s3-bucket]]/model_location/\")\n",
    "test = spark.read.parquet(\"s3://[[your-s3-bucket]]/data_location/\")\n",
    "\n",
    "predictions = saved_model.transform(test)\n",
    "prediction_and_labels = predictions.select(\"label\",\"prediction\").rdd\n",
    "\n",
    "metrics = BinaryClassificationMetrics(prediction_and_labels)\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
